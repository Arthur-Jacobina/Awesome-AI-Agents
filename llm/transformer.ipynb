{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silly implementation of a Transformer\n",
    "Educational purpose implementation of a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Head\n",
    "This class represents an attention head as described in the paper Attention Is All You Need (just like the rest of the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, model_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.k = nn.Linear(model_dim, head_size)\n",
    "        self.q = nn.Linear(model_dim, head_size)\n",
    "        self.v = nn.Linear(model_dim, head_size)\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # Project inputs to queries, keys, and values\n",
    "        q = self.q(q)\n",
    "        k = self.k(k)  \n",
    "        v = self.v(v)\n",
    "        \n",
    "        # MatMul Q and K transpose\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "        \n",
    "        # Scale the scores\n",
    "        scores = scores / math.sqrt(self.head_size)\n",
    "        \n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0)\n",
    "            \n",
    "            if mask.dtype == torch.float and torch.isinf(mask).any():\n",
    "                scores = scores + mask \n",
    "            else:\n",
    "                scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(scores, dim=-1) \n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert model_dim % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = model_dim // num_heads\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(model_dim, self.head_size) for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        self.output_linear = nn.Linear(model_dim, model_dim)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        head_outputs = []\n",
    "        attention_weights = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            head_output, attn_weights = head(query, key, value, mask)\n",
    "            head_outputs.append(head_output)\n",
    "            attention_weights.append(attn_weights)\n",
    "        \n",
    "        concat_output = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        output = self.output_linear(concat_output)\n",
    "        \n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Layer Connection (add + norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(model_dim, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(model_dim, d_ff, dropout)\n",
    "        self.sublayer = nn.ModuleList([SublayerConnection(model_dim, dropout) for _ in range(2)])\n",
    "        self.size = model_dim\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)[0])\n",
    "        x = self.sublayer[1](x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.sublayer = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(3)])\n",
    "        self.size = d_model\n",
    "        \n",
    "    def forward(self, x, memory, src_mask=None, tgt_mask=None):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)[0])\n",
    "        \n",
    "        x = self.sublayer[1](x, lambda x: self.cross_attn(x, memory, memory, src_mask)[0])\n",
    "        \n",
    "        x = self.sublayer[2](x, self.feed_forward)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask=None, tgt_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer model\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n",
    "                 d_ff=2048, num_layers=6, dropout=0.1, max_seq_length=5000):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
    "        \n",
    "        # Building Encoder and Decoder layers\n",
    "        encoder_layer = EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "        decoder_layer = DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "        \n",
    "        # Building full Encoder and Decoder\n",
    "        self.encoder = Encoder(encoder_layer, num_layers)\n",
    "        self.decoder = Decoder(decoder_layer, num_layers)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.generator = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def _init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters with Xavier uniform\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def encode(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        Encode the source sequence\n",
    "        \"\"\"\n",
    "        # Scale embeddings by sqrt(d_model)\n",
    "        src_embedded = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        src_embedded = self.positional_encoding(src_embedded)\n",
    "        return self.encoder(src_embedded, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, memory, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Decode given the encoded source and target sequence\n",
    "        \"\"\"\n",
    "        # Scale embeddings by sqrt(d_model)\n",
    "        tgt_embedded = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded)\n",
    "        return self.decoder(tgt_embedded, memory, src_mask, tgt_mask)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Take in and process source and target sequences.\n",
    "        \n",
    "        Args:\n",
    "            src: source sequence [batch_size, src_seq_len]\n",
    "            tgt: target sequence [batch_size, tgt_seq_len]\n",
    "            src_mask: mask for source sequence [batch_size, 1, src_seq_len] or broadcastable\n",
    "            tgt_mask: mask for target sequence [batch_size, tgt_seq_len, tgt_seq_len] or broadcastable\n",
    "        \n",
    "        Returns:\n",
    "            output probabilities [batch_size, tgt_seq_len, tgt_vocab_size]\n",
    "        \"\"\"\n",
    "        # Encode the source\n",
    "        memory = self.encode(src, src_mask)\n",
    "        \n",
    "        # Decode with the encoded source and target\n",
    "        decoder_output = self.decode(tgt, memory, src_mask, tgt_mask)\n",
    "        \n",
    "        # Generate the output\n",
    "        output = self.generator(decoder_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"\n",
    "        Generate a square causal mask for the decoder.\n",
    "        The masked positions are filled with float('-inf').\n",
    "        Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        # Add batch dimension to be compatible with other masks\n",
    "        return mask.unsqueeze(0)\n",
    "    \n",
    "    def create_pad_mask(self, matrix, pad_idx):\n",
    "        \"\"\"\n",
    "        Create a mask to hide padding tokens.\n",
    "        Returns a mask of shape [batch_size, 1, seq_len] where 1 indicates a valid token\n",
    "        and 0 indicates a padding token.\n",
    "        \"\"\"\n",
    "        # Create a mask that is 1 for non-padding tokens and 0 for padding tokens\n",
    "        # Shape: [batch_size, seq_len]\n",
    "        mask = (matrix != pad_idx).float()\n",
    "        \n",
    "        # Add a dimension to make it compatible with attention scores\n",
    "        # Shape: [batch_size, 1, seq_len]\n",
    "        return mask.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, src_vocab_size=10000, tgt_vocab_size=10000):\n",
    "    batch_size = 32\n",
    "    max_len = 100\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=0.0001, \n",
    "        betas=(0.9, 0.98), \n",
    "        eps=1e-9\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    src = torch.randint(1, src_vocab_size, (batch_size, max_len))\n",
    "    tgt = torch.randint(1, tgt_vocab_size, (batch_size, max_len))\n",
    "    \n",
    "    src_mask = model.create_pad_mask(src, 0)\n",
    "    \n",
    "    tgt_padding_mask = model.create_pad_mask(tgt[:, :-1], 0)\n",
    "    \n",
    "    tgt_look_ahead_mask = model.generate_square_subsequent_mask(tgt[:, :-1].size(1))\n",
    "    \n",
    "    if tgt_padding_mask.device != tgt_look_ahead_mask.device:\n",
    "        tgt_look_ahead_mask = tgt_look_ahead_mask.to(tgt_padding_mask.device)\n",
    "    \n",
    "    if tgt_padding_mask is not None:\n",
    "        expanded_padding_mask = tgt_padding_mask.expand(-1, tgt[:, :-1].size(1), -1)\n",
    "        inf_padding_mask = expanded_padding_mask.float().masked_fill(expanded_padding_mask == 0, float('-inf'))\n",
    "        tgt_mask = torch.minimum(inf_padding_mask, tgt_look_ahead_mask)\n",
    "    else:\n",
    "        tgt_mask = tgt_look_ahead_mask\n",
    "    \n",
    "    output = model(src, tgt[:, :-1], src_mask, tgt_mask)\n",
    "    \n",
    "    loss = criterion(\n",
    "        output.contiguous().view(-1, tgt_vocab_size), \n",
    "        tgt[:, 1:].contiguous().view(-1)\n",
    "    )\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return model, loss\n",
    "\n",
    "\n",
    "def inference_example(model, src, max_len=100, start_symbol=2):\n",
    "    model.eval()\n",
    "    \n",
    "    src_mask = model.create_pad_mask(src, 0)\n",
    "    encoder_output = model.encode(src, src_mask)\n",
    "    \n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src).long()\n",
    "    \n",
    "    for i in range(max_len - 1):\n",
    "        tgt_mask = model.generate_square_subsequent_mask(ys.size(1)).to(src.device)\n",
    "        \n",
    "        out = model.decode(ys, encoder_output, src_mask, tgt_mask)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        \n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        \n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src).fill_(next_word)], dim=1)\n",
    "        \n",
    "        if next_word == 3: \n",
    "            break\n",
    "    \n",
    "    return ys\n",
    "\n",
    "\n",
    "def training_loop(model, train_dataloader, valid_dataloader, n_epochs, src_vocab_size, tgt_vocab_size):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=0.0001, \n",
    "        betas=(0.9, 0.98), \n",
    "        eps=1e-9\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.1, \n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (src, tgt) in enumerate(train_dataloader):\n",
    "            src_mask = model.create_pad_mask(src, 0)\n",
    "            \n",
    "            tgt_padding_mask = model.create_pad_mask(tgt[:, :-1], 0)\n",
    "            tgt_look_ahead_mask = model.generate_square_subsequent_mask(tgt[:, :-1].size(1)).to(src.device)\n",
    "            \n",
    "            expanded_padding_mask = tgt_padding_mask.expand(-1, tgt[:, :-1].size(1), -1)\n",
    "            inf_padding_mask = expanded_padding_mask.float().masked_fill(expanded_padding_mask == 0, float('-inf'))\n",
    "            tgt_mask = torch.minimum(inf_padding_mask, tgt_look_ahead_mask)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt[:, :-1], src_mask, tgt_mask)\n",
    "            \n",
    "            loss = criterion(\n",
    "                output.contiguous().view(-1, tgt_vocab_size), \n",
    "                tgt[:, 1:].contiguous().view(-1)\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, Train loss: {loss.item():.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for src, tgt in valid_dataloader:\n",
    "                src_mask = model.create_pad_mask(src, 0)\n",
    "                tgt_padding_mask = model.create_pad_mask(tgt[:, :-1], 0)\n",
    "                tgt_look_ahead_mask = model.generate_square_subsequent_mask(tgt[:, :-1].size(1)).to(src.device)\n",
    "                \n",
    "                expanded_padding_mask = tgt_padding_mask.expand(-1, tgt[:, :-1].size(1), -1)\n",
    "                inf_padding_mask = expanded_padding_mask.float().masked_fill(expanded_padding_mask == 0, float('-inf'))\n",
    "                tgt_mask = torch.minimum(inf_padding_mask, tgt_look_ahead_mask)\n",
    "                \n",
    "                output = model(src, tgt[:, :-1], src_mask, tgt_mask)\n",
    "                \n",
    "                loss = criterion(\n",
    "                    output.contiguous().view(-1, tgt_vocab_size), \n",
    "                    tgt[:, 1:].contiguous().view(-1)\n",
    "                )\n",
    "                \n",
    "                valid_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        valid_loss /= len(valid_dataloader)\n",
    "        \n",
    "        scheduler.step(valid_loss)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}, Train loss: {train_loss:.4f}, Valid loss: {valid_loss:.4f}')\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best_transformer_model.pt')\n",
    "            print(f'Best model saved with validation loss: {best_valid_loss:.4f}')\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
